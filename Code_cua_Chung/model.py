# =============================================
#  ğŸ§  Huáº¥n luyá»‡n MLP cho nháº­n dáº¡ng ngÃ´n ngá»¯ kÃ½ hiá»‡u tiáº¿ng Viá»‡t
#  Input: train.csv, valid.csv, test.csv (cÃ¹ng thÆ° má»¥c)
#  Output: model_mlp.pkl + scaler.pkl
# =============================================

import pandas as pd
import joblib
import os
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# -----------------------------
# 1ï¸âƒ£ Äá»c dá»¯ liá»‡u (CÃCH CHáº Y á»”N Äá»ŠNH)
# -----------------------------

# --- XÃ¡c Ä‘á»‹nh vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i cá»§a script nÃ y ---
script_dir = os.path.dirname(os.path.abspath(__file__))

# --- Táº¡o Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n cÃ¡c file data ---
train_path = os.path.join(script_dir, r'..\1_data\processed\train_landmarks_augmented.csv')
valid_path = os.path.join(script_dir, r'..\1_data\processed\valid_landmarks.csv')
test_path = os.path.join(script_dir, r'..\1_data\processed\test_landmarks.csv')

# --- Äá»c dá»¯ liá»‡u báº±ng Ä‘Æ°á»ng dáº«n má»›i ---
df_train = pd.read_csv(train_path)
df_valid = pd.read_csv(valid_path)
df_test = pd.read_csv(test_path)

print(f"Train: {len(df_train)} máº«u, Valid: {len(df_valid)}, Test: {len(df_test)}")

# -----------------------------
# 2ï¸âƒ£ Chuáº©n bá»‹ dá»¯ liá»‡u
# -----------------------------
X_train = df_train.drop('label', axis=1)
y_train = df_train['label']

X_valid = df_valid.drop('label', axis=1)
y_valid = df_valid['label']

X_test = df_test.drop('label', axis=1)
y_test = df_test['label']

# -----------------------------
# 3ï¸âƒ£ Chuáº©n hÃ³a dá»¯ liá»‡u
# -----------------------------
scaler = StandardScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled = scaler.transform(X_test)

print("âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a (StandardScaler).")

# -----------------------------
# 4ï¸âƒ£ Huáº¥n luyá»‡n MLP (vá»›i GridSearch nháº¹)
# -----------------------------
param_grid = {
    'hidden_layer_sizes': [(128, 64), (256, 128, 64)],
    'activation': ['relu'],
    'solver': ['adam'],
    'alpha': [1e-4, 1e-3],  # regularization
    'learning_rate_init': [0.001, 0.0005]
}

print("ğŸ” Äang tÃ¬m cáº¥u hÃ¬nh tá»‘t nháº¥t báº±ng GridSearchCV (máº¥t vÃ i phÃºt)...")
mlp = MLPClassifier(max_iter=300, random_state=42)
grid = GridSearchCV(mlp, param_grid, cv=3, verbose=1, n_jobs=-1)
grid.fit(X_train_scaled, y_train)

best_model = grid.best_estimator_
print(f"âœ… Cáº¥u hÃ¬nh tá»‘t nháº¥t: {grid.best_params_}")

# -----------------------------
# 5ï¸âƒ£ ÄÃ¡nh giÃ¡ trÃªn táº­p VALID
# -----------------------------
valid_preds = best_model.predict(X_valid_scaled)
valid_acc = accuracy_score(y_valid, valid_preds)
print(f"\nğŸ¯ Accuracy (VALID): {valid_acc * 100:.2f}%")

print("\nğŸ“Š BÃ¡o cÃ¡o chi tiáº¿t:")
print(classification_report(y_valid, valid_preds))

# -----------------------------
# 6ï¸âƒ£ ÄÃ¡nh giÃ¡ trÃªn táº­p TEST
# -----------------------------
test_preds = best_model.predict(X_test_scaled)
test_acc = accuracy_score(y_test, test_preds)
print(f"\nğŸ§¾ Accuracy (TEST): {test_acc * 100:.2f}%")

# -----------------------------
# 7ï¸âƒ£ Ma tráº­n nháº§m láº«n
# -----------------------------
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, test_preds)
sns.heatmap(cm, annot=False, cmap='Blues')
plt.title('Confusion Matrix - Test Set')
plt.xlabel('Dá»± Ä‘oÃ¡n')
plt.ylabel('Thá»±c táº¿')
plt.show()

# -----------------------------
# 8ï¸âƒ£ LÆ°u model vÃ  scaler
# -----------------------------
# --- Táº¡o Ä‘Æ°á»ng dáº«n lÆ°u file tuyá»‡t Ä‘á»‘i ---
MODEL_PATH = os.path.join(script_dir, r'..\4_models\model_mlp.pkl')
SCALER_PATH = os.path.join(script_dir, r'..\4_models\scaler.pkl')

joblib.dump(best_model, MODEL_PATH)
joblib.dump(scaler, SCALER_PATH)

print(f"\nğŸ’¾ ÄÃ£ lÆ°u model: {MODEL_PATH}")
print(f"ğŸ’¾ ÄÃ£ lÆ°u scaler: {SCALER_PATH}")
print("ğŸš€ Huáº¥n luyá»‡n hoÃ n táº¥t!")